# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.13.8
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# 0. ê°œìš”
# 1. Library & Data Import
# 2. ë°ì´í„°ì…‹ ì‚´í´ë³´ê¸°
# 3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
#     1. ì •ê·œí‘œí˜„ì‹ ì ìš©
#     2. ë§ë­‰ì¹˜ ë§Œë“¤ê¸°
#     3. í† í°í™”
#     4. Word count 
#         - Bag of words
#         - TF-IDF ë³€í™˜
# 4. ê°ì„± ë¶„ì„
#     1. ë°ì´í„°ì…‹ ìƒì„±
#         - Label
#         - Feature
#     2. Training set / Test set ë‚˜ëˆ„ê¸°
#     3. Logistic regression ëª¨ë¸ í•™ìŠµ
# 5. ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ë¶„ì„
# 6. í˜¸í…”ë¦¬ë·° ê°ì„± (ê¸ì •/ë¶€ì •) ì˜ˆì¸¡í•¨ìˆ˜
#     1. Logistic regression
#     2. SVM
#     3. Decision tree
#     4. Random forest
#     5. KNN
#     6. LSTM
#     7. Grid search
#     8. ê° ëª¨ë¸ë³„ AUC score ë¹„êµ
# 7. ê°œì„ ì 

# # 0. ê°œìš”

# https://hyemin-kim.github.io/2020/08/29/E-Python-TextMining-2/#5-%EA%B8%8D%EC%A0%95-%EB%B6%80%EC%A0%95-%ED%82%A4%EC%9B%8C%EB%93%9C-%EB%B6%84%EC%84%9D

# í˜¸í…”ì˜ ë¦¬ë·° ë°ì´í„°(í‰ê°€ ì ìˆ˜ + í‰ê°€ ë‚´ìš©)ì„ í™œìš©í•´ ë‹¤ìŒ 2ê°€ì§€ ë¶„ì„ì„ ì§„í–‰í•œë‹¤.
#
# 1. ë¦¬ë·° ì†ì— ë‹´ê¸´ ì‚¬ëŒì˜ ê¸ì • / ë¶€ì • ê°ì„±ì„ íŒŒì•…í•˜ì—¬ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ê°ì„± ë¶„ë¥˜ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“ ë‹¤.
#
# 2. ë§Œë“  ëª¨ë¸ì„ í™œìš©í•´ ê¸ì • / ë¶€ì • í‚¤ì›Œë“œë¥¼ ì¶œë ¥í•´, ì´ìš©ê°ë“¤ì´ ëŠë‚€ í˜¸í…”ì˜ ì¥,ë‹¨ì ì„ íŒŒì•…í•œë‹¤.

# # 1. Library & Data Import

# +
# %matplotlib inline

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')
# -

df =pd.read_csv("C:/Users/gilon/data/hotel_review.csv")

df

# # 2. ë°ì´í„°ì…‹ ì‚´í´ë³´ê¸°

df.head()

df.shape

# ê²°ì¸¡ì¹˜
df.isnull().sum()

# information
df.info()

# content í™•ì¸ : ì²«ë²ˆì§¸ ë¦¬ë·° í™•ì¸
df['content'][0]

# content í™•ì¸ : 101ë²ˆì§¸ ë¦¬ë·° í™•ì¸
df['content'][100]

del df['Unnamed: 0']

df

#ë³¸ë¬¸ì˜ ì œëª©ê³¼ ë‚´ìš© í•©ì¹˜ê¸°
df['review']=df['Title']+" "+df['content']

df

df = df.drop(['Title', 'content'], axis=1)

df

# Text Miningì„ ì ìš©í•  í•„ìš”ê°€ ì—†ëŠ” ë¬¸ìë“¤ì€ ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•´ì„œ ì œê±°í•œë‹¤.
#
# 1. ê°œí–‰ ë¬¸ì(\n) ì œê±°
# 2. íŠ¹ìˆ˜ë¬¸ì(! ? , .) ì œê±°
# 3. ì´ëª¨í‹°ì½˜ ì œê±° -> ì´ëª¨í‹°ì½˜ë„ ê°ì •ì„ í‘œí˜„í•˜ëŠ” í•˜ë‚˜ì˜ ë°©ì‹ì´ê¸° ë•Œë¬¸ì— í…ìŠ¤íŠ¸ë¡œ ëŒ€ì¹˜í•´ì£¼ëŠ” ë°©ì‹ì„ ì ìš©í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì˜¬ë¦´ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨ 
#     - ì˜ˆì‹œ: (ğŸ˜-> (í•˜íŠ¸) / ğŸ‘ğŸ»-> (êµ¿) )
# 4. ë„ì–´ì“°ê¸°, ë§ì¶¤ë²• ê²€ì‚¬

# # 3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

# <í…ìŠ¤íŠ¸ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´ì„œ ë¬´ì—‡ì„ í•´ì•¼ í• ê¹Œ?>
# 1. ì „ì²˜ë¦¬ ì‘ì—… : Tokenization(ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ ìª¼ê°œê¸°), ë¶ˆìš©ì–´ ì œê±°, ë‹¨ì–´ ì •ê·œí™”(ex. apples â†’ apple) ë“±ì˜ ì „ì²˜ë¦¬ ì‘ì—…
# 2. ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¡œ ë³€í™˜ : ë¬¸ìë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰ -> BoW(Bag of Words)

# ## 3-1. ì •ê·œ í‘œí˜„ì‹ ì ìš© 
# ì˜ì–´ê°€ ì•„ë‹Œ ë¬¸ìëŠ” ì œê±°í•˜ë©° ë‹¨ì–´ë§Œ ë‚¨ê¸°ë„ë¡ í•œë‹¤.

import re

# 1. ì˜ë¬¸ì ì´ì™¸ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

only_english = re.sub('[^a-zA-Z]', ' ', df['review'][0])

df['review'][0]

# 2. ëŒ€ë¬¸ìëŠ” ì†Œë¬¸ìë¡œ ë³€í™˜
#
# ì˜ì–´ì˜ ê²½ìš° ë¬¸ì¥ì˜ ì‹œì‘ì´ë‚˜ ê³ ìœ ëª…ì‚¬ëŠ” ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ì—¬ ë¶„ì„í• ë•Œ "Apple"ê³¼ "apple"ì„ ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì·¨ê¸‰í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ëª¨ë“  ë‹¨ì–´ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•œë‹¤.

# ì†Œë¬¸ì ë³€í™˜
no_capitals = only_english.lower().split()

# 3. ë¶ˆìš©ì–´ ì œê±°
#
#     * ë¶ˆìš©ì–´ : í•™ìŠµ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ì´ë‚˜ í•™ìŠµì— ì‹¤ì œë¡œ ê¸°ì—¬í•˜ì§€ ì•ŠëŠ” í…ìŠ¤íŠ¸
#
# I, that, is, the, a  ë“±ê³¼ ê°™ì´ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì´ì§€ë§Œ ì‹¤ì œë¡œ ì˜ë¯¸ë¥¼ ì°¾ëŠ”ë° ê¸°ì—¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì„ ì œê±°í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤.

from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')

# ë¶ˆìš©ì–´ ì œê±°
stops = set(stopwords.words('english'))
no_stops = [word for word in no_capitals if not word in stops]

# 4. ì–´ê°„ ì¶”ì¶œ
#
#     * see, saw, seen
#     * run, running, ran
#
# ìœ„ ì˜ˆì‹œì²˜ëŸ¼ ì–´í˜•ì´ ê³¼ê±°í˜•ì´ë“  ë¯¸ë˜í˜•ì´ë“  í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ì·¨ê¸‰í•˜ê¸° ìœ„í•œ ì²˜ë¦¬ì‘ì—…ì´ë‹¤.
#
# nltkì—ì„œ ì œê³µí•˜ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ì—¬ëŸ¬ê°€ì§€ ì–´ê°„ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜(Porter, Lancaster, Snowball ë“±ë“±)ì´ ì¡´ì¬í•œë‹¤.

# ì–´ê°„ ì¶”ì¶œ
stemmer = nltk.stem.SnowballStemmer('english')
stemmer_words = [stemmer.stem(word) for word in no_stops]


# **ì „ì²´ ì½”ë“œ**

def data_text_cleaning(data):
 
    # ì˜ë¬¸ì ì´ì™¸ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ë³€í™˜
    only_english = re.sub('[^a-zA-Z]', ' ', data)
 
    # ì†Œë¬¸ì ë³€í™˜
    no_capitals = only_english.lower().split()
 
    # ë¶ˆìš©ì–´ ì œê±°
    stops = set(stopwords.words('english'))
    no_stops = [word for word in no_capitals if not word in stops]
 
    # ì–´ê°„ ì¶”ì¶œ
    stemmer = nltk.stem.SnowballStemmer('english')
    stemmer_words = [stemmer.stem(word) for word in no_stops]
 
    # ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ì—¬ ê²°ê³¼ ë°˜í™˜
    return ' '.join(stemmer_words)


# ì²«ë²ˆì§¸ ë¦¬ë·°
df['review'][0]

# ì²«ë²ˆì§¸ ë¦¬ë·°ì— ëŒ€í•œ ì •ê·œ í‘œí˜„ì‹ í•¨ìˆ˜(data_text_cleaning) ì ìš©
data_text_cleaning(df['review'][0])

# ## 3-2. ë§ë­‰ì¹˜ ë§Œë“¤ê¸°

# ë§ë­‰ì¹˜ ìƒì„±
words = "".join(df['review'].tolist())
words

# ì „ì²´ ë§ë­‰ì¹˜(corpus)ì— ì •ê·œ í‘œí˜„ì‹ í•¨ìˆ˜(data_text_cleaning) ì ìš©

corpus = data_text_cleaning(words)
corpus

# ## 3-3. í† í°í™” 
#
# ë¬¸ìì—´ì—ì„œ ë‹¨ì–´ë¡œ ë¶„ë¦¬ì‹œí‚¨ë‹¤.

from nltk.tokenize import word_tokenize

word_token = word_tokenize(corpus)
word_token

# í•´ë‹¹ ë¦¬ë·° ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜
len(word_token) 

# **ê° ë‹¨ì–´ë“¤ì˜ ë¹ˆë„ íƒìƒ‰**

from collections import Counter

counter = Counter(word_token)

counter.most_common(10)

# ## 3-4.  Word Count

# **1. BoW ë²¡í„° ìƒì„±**
#
# https://doitgrow.com/15

# ë°ì´í„° í˜•íƒœë¥¼ í†µì¼í•˜ê¸° ìœ„í•´ ì „ì²´ ë¬¸ì„œ(ë˜ëŠ” ë¬¸ì¥)ë¥¼ ë³´ê³  ë‹¨ì–´ ê°€ë°©(BoW)ë¥¼ ì„¤ê³„/ì œì‘í•œë‹¤.

len(word_token)

# ì „ì²´ ë¦¬ë·°ë¥¼ í†µí•´ 89647ê°œì˜ ê³ ìœ  ë‹¨ì–´ë“¤ì´ ìˆëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤. 

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
cv.fit(df['review'].tolist()) # ê°€ì§€ê³  ìˆëŠ” ë¬¸ì¥ë“¤ë¡œ ê°€ë°©ì„ ì„¤ê³„
vectors = cv.transform(df['review'].tolist()).toarray() # ë‹¨ì–´ë“¤ì„ ê°€ë°©ì— ì •ë¦¬í•˜ì—¬ ë„£ìŒ

print(vectors)

vectors.shape

# **2. TF-IDF(Term Frequency-Inverse Document Frequency) ì ìš©**

# BoWì˜ í•œê³„ì ê³¼ ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ Bag of Words ë²¡í„°ì— ëŒ€í•´ì„œ TF-IDFë³€í™˜ì„ ì§„í–‰í•œë‹¤.
# * í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ BoWë¥¼ í†µí•´ ì–¸ì–´ ëª¨ë¸ë¡œ í•´ì„í•˜ë ¤ê³  í•œë‹¤ë©´ ëª‡ ê°€ì§€ ë¬¸ì œì ì´ ì¡´ì¬í•œë‹¤.
#
# 1) ë¶ˆìš©ì–´(ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´)ë¥¼ ì œëŒ€ë¡œ ì œê±°í•˜ì§€ ëª»í•˜ë©´ ì›í•˜ì§€ ì•ŠëŠ” í¸í–¥ëœ(biased) ê²°ê³¼ê°€ ì–»ì–´ì§ˆ ìˆ˜ ìˆë‹¤. -> TF-IDF ëª¨ë¸ë¡œ í•´ê²°
#
# 2) ë¬¸ì¥(ë˜ëŠ” ë¬¸ì„œ)ì˜ ì˜ë¯¸ê°€ ë‹¨ì–´ ìˆœì„œì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì§€ë§Œ BoW ëª¨ë¸ì€ ì´ë¥¼ ë°˜ì˜í•  ìˆ˜ ì—†ë‹¤. -> TF-IDF ëª¨ë¸ë¡œë„ í•´ê²° ë¶ˆê°€ëŠ¥, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ n-gramì„ ì‚¬ìš©í•˜ê¸°ë„ í•˜ì§€ë§Œ n-gramì„ ì‚¬ìš©í•˜ë©´ ë²¡í„°ì˜ ì°¨ì›ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ê±°ë‚˜, ë²¡í„°ê°€ í¬ì†Œ(Sparse) í˜•íƒœë¡œ í‘œí˜„ë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì ¸ì„œ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤.

# **TF-IDF ë°©ë²•**
#
# ë¬¸ì„œë¥¼ íŠ¹ì§•ì§“ëŠ” ì¤‘ìš”í•œ ë‹¨ì–´ëŠ” ë„ˆë¬´ ì ê²Œ ë‚˜ì˜¤ì§€ë„ ë„ˆë¬´ ë§ì´ ë‚˜ì˜¤ì§€ë„ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ë„ˆë¬´ ë§ì´ ì¶œí˜„í–ˆê±°ë‚˜ ë„ˆë¬´ ì ê²Œ ì¶œí˜„í•œ ë‹¨ì–´ë“¤ì—ëŠ” íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•˜ê³ , ì ì ˆí•˜ê²Œ ì¶œí˜„í•œ ë‹¨ì–´ë“¤ì—ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ë¬¸ì„œì˜ íŠ¹ì§•ì„ ë” ë§ì´ ëŒ€ë³€í•˜ë„ë¡ ë§Œë“¤ì–´ ì¤€ë‹¤.

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_vectorizer = TfidfTransformer()
tf_idf_vect = tfidf_vectorizer.fit_transform(vectors)

print(tf_idf_vect.shape)

# * í•œ í–‰(row)ì€ í•œ ë¦¬ë·°ë¥¼ ì˜ë¯¸ : 2889
# * í•œ ì—´(column)ì€ í•œ ë‹¨ì–´ë¥¼ ì˜ë¯¸ : 7282

# ì²« ë²ˆì§¸ ë¦¬ë·°ì—ì„œì˜ ë‹¨ì–´ ì¤‘ìš”ë„(TF-IDF ê°’) -- 0ì´ ì•„ë‹Œ ê²ƒë§Œ ì¶œë ¥
print(tf_idf_vect[0])

# ì²« ë²ˆì§¸ ë¦¬ë·°ì—ì„œ ëª¨ë“  ë‹¨ì–´ì˜ ì¤‘ìš”ë„ -- 0ì¸ ê°’ê¹Œì§€ í¬í•¨
print(tf_idf_vect[0].toarray().shape)
print(tf_idf_vect[0].toarray())

cv.vocabulary_

# # 4. ê°ì„± ë¶„ì„

# ê°ì„± ë¶„ì„ ì˜ˆì¸¡ ëª¨ë¸ : ì´ìš©ì ë¦¬ë·°ì˜ í‰ê°€ ë‚´ìš©ì„ í†µí•´ ì´ ë¦¬ë·°ê°€ ê¸ì •ì ì¸ì§€ ë˜ëŠ” ë¶€ì •ì ì¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ì—¬, ì´ìš©ìì˜ ê°ì„±ì„ íŒŒì•…í•œë‹¤.
# * xê°’ (feature ê°’) = ì´ìš©ì ë¦¬ë·°ì˜ í‰ê°€ ë‚´ìš©
# * yê°’ (label ê°’) = ì´ìš©ìì˜ ê¸/ë¶€ì • ê°ì„±

# ## 4-1. ë°ì´í„°ì…‹ ìƒì„±

# **1. Label**

# ì´ìš©ìì˜ ê°ì„±ì„ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ”â€œscoreâ€ë³€ìˆ˜ëŠ” 1 ~ 5ì˜ valueë¥¼ ê°€ì§„ë‹¤. ë”°ë¼ì„œ "score" ë³€ìˆ˜ (rating: 1 ~ 5)ë¥¼ ì´ì§„ ë³€ìˆ˜ (ê¸ì •: 1, ë¶€ì •:0)ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤.

df

df['review'][566]

df.sample(10) # ë¬´ì‘ìœ„ë¡œ 10ê°œì˜ ìƒ˜í”Œì„ ì¶”ì¶œ

# ë¦¬ë·° ë‚´ìš©ì™€ í‰ì ì„ ì‚´í´ë³´ë©´, 4 ~ 5ì  ë¦¬ë·°ëŠ” ëŒ€ë¶€ë¶„ ê¸ì •ì ì´ì—ˆì§€ë§Œ, 1 ~ 3ì  ë¦¬ë·°ì—ì„œëŠ” ë¶€ì •ì ì¸ í‰ê°€ê°€ ëŒ€ë¶€ë¶„ì´ì—ˆë‹¤.
# ë”°ë¼ì„œ 4ì , 5ì ì¸ ë¦¬ë·°ëŠ” "ê¸ì •ì ì¸ ë¦¬ë·°"ë¡œ ë¶„ë¥˜í•˜ì—¬ 1ì„ ë¶€ì—¬í•˜ê³ , 1 ~ 3ì  ë¦¬ë·°ëŠ” "ë¶€ì •ì ì¸ ë¦¬ë·°"ë¡œ ë¶„ë¥˜í•˜ì—¬ 0ì„ ë¶€ì—¬í•˜ë„ë¡ í•œë‹¤.

df['score'].hist()


# +
def rating_to_label(rating):
    if rating > 3:
        return 1
    else:
        return 0
    
df['y'] = df['score'].apply(lambda x: rating_to_label(x))
# -

df.head()

df["y"].value_counts()

# **2. Feature**

# ëª¨ë¸ì˜ Feature ë³€ìˆ˜ëŠ” ë¦¬ë·°ì—ì„œ ì¶”ì¶œëœ í˜•íƒœì†Œì™€ ê·¸ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚˜ëŠ” tf_idf_vectë¡œ ëŒ€ì²´í•œë‹¤.

# ## 4-2. Training set / Test set ë‚˜ëˆ„ê¸°

from sklearn.model_selection import train_test_split

x = tf_idf_vect
y = df['y']

x.shape

y.shape

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=1)

x_train.shape, y_train.shape

x_test.shape, y_test.shape

# ## 4-3. ëª¨ë¸ í•™ìŠµ

# ### 1. Logistic Regression ëª¨ë¸ í•™ìŠµ

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# fit in training set
lr = LogisticRegression(random_state = 0)
lr.fit(x_train, y_train)

# predict in test set
y_pred = lr.predict(x_test)

# ë¶„ë¥˜ ê²°ê³¼ í‰ê°€

# +
# classification result for test set

print('accuracy: %.2f' % accuracy_score(y_test, y_pred))
print('precision: %.2f' % precision_score(y_test, y_pred))
print('recall: %.2f' % recall_score(y_test, y_pred))
print('F1: %.2f' % f1_score(y_test, y_pred))

# +
# confusion matrix

from sklearn.metrics import confusion_matrix

confu = confusion_matrix(y_true = y_test, y_pred = y_pred)

plt.figure(figsize=(4, 3))
sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')
plt.title('Confusion Matrix')
plt.show()
# -

# ëª¨ë¸ í‰ê°€ê²°ê³¼ë¥¼ ì‚´í´ë³´ë©´, ëª¨ë¸ì´ ì§€ë‚˜ì¹˜ê²Œ ê¸ì •(â€œ1â€)ìœ¼ë¡œë§Œ ì˜ˆì¸¡í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. ë”°ë¼ì„œ ê¸ì • ë¦¬ë·°ëŠ” ì˜ ì˜ˆì¸¡í•˜ì§€ë§Œ, ë¶€ì • ë¦¬ë·°ì— ëŒ€í•œ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë§¤ìš° ë‚®ë‹¤. ì´ëŠ” ìƒ˜í”Œë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•ìœ¼ë¡œ ì¸í•œ ë¬¸ì œë¡œ ë³´ì´ë¯€ë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì¡°ì •ì„ ì§„í–‰í•œë‹¤.

# ### 2. ìƒ˜í”Œë§ ì¬ì¡°ì •

df['y'].value_counts()

positive_random_idx = df[df['y']==1].sample(819, random_state=1).index.tolist()
negative_random_idx = df[df['y']==0].sample(819, random_state=1).index.tolist()
# random_state : random í•¨ìˆ˜ì˜ seedê°’ -> random ê°’ì„ ê³ ì •

random_idx = positive_random_idx + negative_random_idx
x = tf_idf_vect[random_idx]
y = df['y'][random_idx]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=12)

x_train.shape, y_train.shape

x_test.shape, y_test.shape

# ### 3. Logistic Regression ëª¨ë¸ ì¬í•™ìŠµ

lr2 = LogisticRegression(random_state = 0)
lr2.fit(x_train, y_train)
y_pred = lr2.predict(x_test)

# ë¶„ë¥˜ ê²°ê³¼ í‰ê°€

# +
# classification result for test set

print('accuracy: %.2f' % accuracy_score(y_test, y_pred))
print('precision: %.2f' % precision_score(y_test, y_pred))
print('recall: %.2f' % recall_score(y_test, y_pred))
print('F1: %.2f' % f1_score(y_test, y_pred))

# +
# confusion matrix

from sklearn.metrics import confusion_matrix

confu = confusion_matrix(y_true = y_test, y_pred = y_pred)

plt.figure(figsize=(4, 3))
sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')
plt.title('Confusion Matrix')
plt.show()
# -

# ì´ì œ ëª¨ë¸ì´ â€œê¸ì •ì ì¸â€ ì¼€ì´ìŠ¤ì™€ â€œë¶€ì •ì ì¸â€ ì¼€ì´ìŠ¤ë¥¼ ëª¨ë‘ ì ë‹¹íˆ ì˜ ë§ì¶˜ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

# # 5. ê¸ì • / ë¶€ì • í‚¤ì›Œë“œ ë¶„ì„

# Logistic Regression ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ê¸/ë¶€ì • í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•œë‹¤.
#
# ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ í†µí•´ì„œ ì´ìš©ìê°€ ëŠë¼ëŠ” í˜¸í…”ì˜ ì¥,ë‹¨ì ì„ íŒŒì•…í•  ìˆ˜ ìˆê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•ìœ¼ë¡œ ìœ ì§€í•´ì•¼ í•  ì¢‹ì€ ì„œë¹„ìŠ¤ì™€ ê°œì„ ì´ í•„ìš”í•œ ì•„ì‰¬ìš´ ì„œë¹„ìŠ¤ì— ëŒ€í•´ì„œë„ ì–´ëŠì •ë„ íŒë‹¨í•  ìˆ˜ ìˆë‹¤.

# Logistic Regression ëª¨ë¸ì— ê° ë‹¨ì–´ì˜ coeficientë¥¼ ì‹œê°í™”

lr2.coef_

# +
# print logistic regression's coef

plt.figure(figsize=(10, 8))
plt.bar(range(len(lr2.coef_[0])), lr2.coef_[0])
# -

# * ê³„ìˆ˜ê°€ ì–‘ì¸ ê²½ìš° : ë‹¨ì–´ê°€ ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì³¤ë‹¤
# * ê³„ìˆ˜ê°€ ìŒì¸ ê²½ìš° : ë‹¨ì–´ê°€ ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì³¤ë‹¤
#     
# -> ì´ ê³„ìˆ˜ë“¤ì„ í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬í•˜ë©´, ê¸ì • / ë¶€ì • í‚¤ì›Œë“œë¥¼ ì¶œë ¥í•˜ëŠ” ì§€í‘œê°€ ëœë‹¤.

#  "ê¸ì • í‚¤ì›Œë“œ"ì™€ "ë¶€ì • í‚¤ì›Œë“œ"ì˜ Top 5ë¥¼ ê°ê° ì¶œë ¥

print(sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)[:5])
print(sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)[-5:])
# enumerate: ì¸ë±ìŠ¤ ë²ˆí˜¸ì™€ ì»¬ë ‰ì…˜ì˜ ì›ì†Œë¥¼ tupleí˜•íƒœë¡œ ë°˜í™˜í•¨ : ë‹¨ì–´ì˜ coeficientì™€ indexê°€ ì¶œë ¥

# ì „ì²´ ë‹¨ì–´ê°€ í¬í•¨í•œ "ê¸ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"ì™€ "ë¶€ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"ë¥¼ ì •ì˜í•˜ê³  ì¶œë ¥

coef_pos_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)
coef_neg_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = False)
coef_pos_index

# indexë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜í•˜ì—¬ "ê¸ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"ì™€ "ë¶€ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"ì˜ Top 20 ë‹¨ì–´ë¥¼ ì¶œë ¥

# +
invert_index_vectorizer = {v: k for k, v in cv.vocabulary_.items()}
invert_index_vectorizer

# invert_index_vectorizer = {v: k for k, v in word_token.to}
# invert_index_vectorizer
# -

len(invert_index_vectorizer)

for coef in coef_pos_index[:20]:
    print(invert_index_vectorizer[coef[1]], coef[0])

# **ê²°ë¡  : ì´ìš©ê°ë“¤ì´ ì£¼ë¡œ í˜¸í…”ì˜ ì²­ê²°ë„(clean)ì™€ ì•ˆë½í•¨(comfortable)ì— ë§Œì¡±í•˜ì˜€ë‹¤.**

for coef in coef_neg_index[:20]:
    print(invert_index_vectorizer[coef[1]], coef[0])


# **ê²°ë¡  : ì´ìš©ê°ë“¤ì´ ì£¼ë¡œ í˜¸í…”ì´ ì²­ê²°í•˜ì§€ ì•Šìœ¼ë©° ì˜¤ë˜ëê³  ì„œë¹„ìŠ¤ê°€ ë¬´ë¡€í•˜ë‹¤ë©° ë¶ˆë§Œì¡±í•˜ì˜€ë‹¤.**

# # 6. í˜¸í…”ë¦¬ë·° ê°ì„± (ê¸ì •/ë¶€ì •) ì˜ˆì¸¡í•¨ìˆ˜

# ## 6-1. Logistic Regression

def sentiment_predict_lr(input_text):

    #ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰
    input_text = data_text_cleaning(input_text)
    #input_text = ["".join(input_text)]

    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ í”¼ì²˜ ë²¡í„°í™”
    st_tfidf = cv.transform([input_text])
    tf_idf_vect = tfidf_vectorizer.transform(st_tfidf)
    tf_idf_vect = tf_idf_vect.toarray()

    # ìµœì  ê°ì„± ë¶„ì„ ëª¨ë¸ì— ì ìš©í•˜ì—¬ ê°ì„± ë¶„ì„ í‰ê°€
    score = float(lr2.predict(tf_idf_vect))

    #ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
    if(score > 0.5):
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ê¸ì • ê°ì„±')
    else :
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ë¶€ì • ê°ì„±')


sentiment_predict_lr('it was nice')

sentiment_predict_lr('it is not bad price')

sentiment_predict_lr('This was the second time we stayed at Bellagio in 6 months. We love the location. Easy access on foot to go shopping, dining, or shows. Room is very clean and great space. Many restaurants and high end shopping to enjoy. Several nice restaurants in the hotel. Yellowtail Restaurant... yummy!!!!')

#3ì  ë¦¬ë·°
sentiment_predict_lr('I stayed in the Bellagio suite and still the house keeping service as well as the service desk fulfillment of requests was very bad. If you miss the regular room cleaning scheduleâ€¦your room will not be cleaned even if you request multiple times')

#3ì  ë¦¬ë·°
sentiment_predict_lr('Hotel over all is very nice. Aracelis went above and beyond to assist us...... Im a Amex Platinum card holder, an MGM rewards member, had reservations at Prime and Lago were we spend well over 1000 dollars. At the time of arrival, the person who helped us (Rosemary from Philippines) was not to helpful. She offered an upgarde to another room for an additional 450$ per night, even though as an Amex Platinum member we are supposed to receive this at no additional cost when available. On the second day of our stay, Aracelis was able to provide the service that was rightfully expected. In the original room that we were assigned (22 604) there was mold in the shower, dust on the furniture, drawers were dirty, shower head sprayed everywhere, there was glue and stains in the wall paper behind the bed specially. The quality of the toiletries was terrible')

# * comment : 3ì ë¦¬ë·°ì— ëŒ€í•œ ì •í™•í•œ ê°ì„±(ê¸ì •/ë¶€ì •) ì˜ˆì¸¡ì„ ìœ„í•´ì„œ ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•¨ 

# AUC Score í™•ì¸
from sklearn.metrics import roc_auc_score

# +
# classification result for test set
print('accuracy: %.2f' % accuracy_score(y_test, y_pred))
print('precision: %.2f' % precision_score(y_test, y_pred))
print('recall: %.2f' % recall_score(y_test, y_pred))
print('F1: %.2f' % f1_score(y_test, y_pred))

AUC_lr = roc_auc_score(y_test,y_pred)
print('AUC: %.8f' % AUC_lr)
# -

# ### ê·¸ë¦¬ë“œ ì„œì¹˜
#
# í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê°œì„  
#
# -> ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œì„œ ê´€ì‹¬ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•©ì„ ì‹œë„

# ë°ì´í„°ë¥¼ train, validation, test setìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë°©ë²•ì€ ì„±ëŠ¥ì´ ì¢‹ê³  ë„ë¦¬ ì‚¬ìš©ëœë‹¤.
# ê·¸ëŸ¬ë‚˜ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ë°©ë²•ì— ë§¤ìš° ë¯¼ê°í•˜ë¯€ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë” ì˜ í‰ê°€í•˜ê¸° ìœ„í•´ì„œëŠ” í›ˆë ¨ì„¸íŠ¸ì™€ ê²€ì •ì„¸íŠ¸ë¥¼ í•œë²ˆë§Œ ë‚˜ëˆ„ì§€ ì•Šê³  êµì°¨ ê²€ì¦(cross validation)ì„ ì‚¬ìš©í•´ì„œ ê° ë§¤ê°œë³€ìˆ˜ì˜ ì¡°í•©ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆë‹¤.
#
# * í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ì„ ìë™í™”í•´ ì£¼ëŠ” ë„êµ¬
# * íƒìƒ‰í•  ë§¤ê°œë³€ìˆ˜ë¥¼ ë‚˜ì—´í•˜ë©´ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•˜ì—¬ ê°€ì¥ ì¢‹ì€ ê²€ì¦ ì ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜ ì¡°í•©ì„ ì„ íƒ, ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ ë§¤ê°œë³€ìˆ˜ ì¡°í•©ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ì„ í›ˆë ¨

from sklearn.model_selection import GridSearchCV # í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™”

clf = LogisticRegression(random_state=0)
params = {'C': [15, 18, 19, 20, 22]}
grid_cv = GridSearchCV(clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)
grid_cv.fit(x_train, y_train)

# ìµœì ì˜ í‰ê°€ íŒŒë¼ë¯¸í„°ëŠ” grid_cv.best_estimator_ì— ì €ì¥ë¨
print(grid_cv.best_params_, grid_cv.best_score_)# ê°€ì¥ ì í•©í•œ íŒŒë¼ë©”í„°, ìµœê³  ì •í™•ë„ í™•ì¸

# ## 6-2. SVM

# +
from sklearn.svm import SVC

svm=SVC()
svm.fit(x_train,y_train)


# -

def sentiment_predict_svm(input_text):

    #ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰
    input_text = data_text_cleaning(input_text)
    #input_text = ["".join(input_text)]

    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ í”¼ì²˜ ë²¡í„°í™”
    st_tfidf = cv.transform([input_text])
    tf_idf_vect = tfidf_vectorizer.transform(st_tfidf)

    # ìµœì  ê°ì„± ë¶„ì„ ëª¨ë¸ì— ì ìš©í•˜ì—¬ ê°ì„± ë¶„ì„ í‰ê°€
    st_predict = float(svm.predict(st_tfidf))

    #ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
    if(st_predict == 0):
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ë¶€ì • ê°ì„±')
    else :
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ê¸ì • ê°ì„±')


sentiment_predict_svm('it was nice')

sentiment_predict_svm('Hotel over all is very nice. Aracelis went above and beyond to assist us...... Im a Amex Platinum card holder, an MGM rewards member, had reservations at Prime and Lago were we spend well over 1000 dollars. At the time of arrival, the person who helped us (Rosemary from Philippines) was not to helpful. She offered an upgarde to another room for an additional 450$ per night, even though as an Amex Platinum member we are supposed to receive this at no additional cost when available. On the second day of our stay, Aracelis was able to provide the service that was rightfully expected. In the original room that we were assigned (22 604) there was mold in the shower, dust on the furniture, drawers were dirty, shower head sprayed everywhere, there was glue and stains in the wall paper behind the bed specially. The quality of the toiletries was terrible')

sentiment_predict_svm('Subpar Dirty hallways and less than average room.')

# predict in test set
y_pred_svm = svm.predict(x_test)

# +
# classification result for test set
print('accuracy: %.2f' % accuracy_score(y_test, y_pred_svm))
print('precision: %.2f' % precision_score(y_test, y_pred_svm))
print('recall: %.2f' % recall_score(y_test, y_pred_svm))
print('F1: %.2f' % f1_score(y_test, y_pred_svm))

AUC_svm = roc_auc_score(y_test,y_pred_svm)
print('AUC: %.8f' % AUC_svm)
# -

# ### ê·¸ë¦¬ë“œ ì„œì¹˜

clf = SVC(random_state=0)
params = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma' : [0.001, 0.01, 0.1, 1, 10, 100]}
grid_cv = GridSearchCV(clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)
grid_cv.fit(x_train, y_train)

# ìµœì ì˜ í‰ê°€ íŒŒë¼ë¯¸í„°ëŠ” grid_cv.best_estimator_ì— ì €ì¥ë¨
print(grid_cv.best_params_, grid_cv.best_score_)# ê°€ì¥ ì í•©í•œ íŒŒë¼ë©”í„°, ìµœê³  ì •í™•ë„ í™•ì¸

# ## 6-3. Decision Tree

# +
from sklearn.tree import DecisionTreeRegressor

dt=DecisionTreeRegressor()
dt.fit(x_train,y_train)


# -

def sentiment_predict_dt(input_text):

    #ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰
    input_text = data_text_cleaning(input_text)
    #input_text = ["".join(input_text)]

    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ í”¼ì²˜ ë²¡í„°í™”
    st_tfidf = cv.transform([input_text])
    tf_idf_vect = tfidf_vectorizer.transform(st_tfidf)

    # ìµœì  ê°ì„± ë¶„ì„ ëª¨ë¸ì— ì ìš©í•˜ì—¬ ê°ì„± ë¶„ì„ í‰ê°€
    st_predict = float(dt.predict(st_tfidf))

    #ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
    if(st_predict > 0.5):
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ë¶€ì • ê°ì„±')
    else :
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ê¸ì • ê°ì„±')


sentiment_predict_dt('it was nice')

sentiment_predict_dt('Hotel over all is very nice. Aracelis went above and beyond to assist us...... Im a Amex Platinum card holder, an MGM rewards member, had reservations at Prime and Lago were we spend well over 1000 dollars. At the time of arrival, the person who helped us (Rosemary from Philippines) was not to helpful. She offered an upgarde to another room for an additional 450$ per night, even though as an Amex Platinum member we are supposed to receive this at no additional cost when available. On the second day of our stay, Aracelis was able to provide the service that was rightfully expected. In the original room that we were assigned (22 604) there was mold in the shower, dust on the furniture, drawers were dirty, shower head sprayed everywhere, there was glue and stains in the wall paper behind the bed specially. The quality of the toiletries was terrible')

sentiment_predict_dt('Subpar Dirty hallways and less than average room.')

sentiment_predict_dt('Dirty Room Stayed at the Bellagio for the first time ever for a work trip.')

# predict in test set
y_pred_dt = dt.predict(x_test)

# +
# classification result for test set
print('accuracy: %.2f' % accuracy_score(y_test, y_pred_dt))
print('precision: %.2f' % precision_score(y_test, y_pred_dt))
print('recall: %.2f' % recall_score(y_test, y_pred_dt))
print('F1: %.2f' % f1_score(y_test, y_pred_dt))

AUC_dt = roc_auc_score(y_test,y_pred_dt)
print('AUC: %.8f' % AUC_dt)
# -

# ### ê·¸ë¦¬ë“œ ì„œì¹˜

clf = DecisionTreeRegressor(random_state=0)
# min_impurity_decrease : ìµœì†Œ ë¶ˆìˆœë„(ì§€ë‹ˆê³„ìˆ˜)ê°€ ë‚®ë‹¤ = ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ë‹¤ = ë°ì´í„°ì˜ ê· ì¼ë„ê°€ ë†’ë‹¤  
params = {"max_depth": [1, 5, 10],
          "min_samples_split": [2, 3],
          "min_impurity_decrease": [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
grid_cv = GridSearchCV(clf, param_grid=params)
grid_cv.fit(x_train, y_train)

# ìµœì ì˜ í‰ê°€ íŒŒë¼ë¯¸í„°ëŠ” grid_cv.best_estimator_ì— ì €ì¥ë¨
print(grid_cv.best_params_, grid_cv.best_score_)# ê°€ì¥ ì í•©í•œ íŒŒë¼ë©”í„°, ìµœê³  ì •í™•ë„ í™•ì¸

# ## 6-4. Random Forest

# +
from sklearn.ensemble import RandomForestClassifier

rf=RandomForestClassifier()
rf.fit(x_train,y_train)


# -

def sentiment_predict_rf(input_text):

    #ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰
    input_text = data_text_cleaning(input_text)
    #input_text = ["".join(input_text)]

    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ í”¼ì²˜ ë²¡í„°í™”
    st_tfidf = cv.transform([input_text])
    tf_idf_vect = tfidf_vectorizer.transform(st_tfidf)

    # ìµœì  ê°ì„± ë¶„ì„ ëª¨ë¸ì— ì ìš©í•˜ì—¬ ê°ì„± ë¶„ì„ í‰ê°€
    st_predict = float(rf.predict(st_tfidf))

    #ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
    if(st_predict > 0.5):
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ë¶€ì • ê°ì„±')
    else :
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ê¸ì • ê°ì„±')


sentiment_predict_rf('Hotel over all is very nice. Aracelis went above and beyond to assist us...... Im a Amex Platinum card holder, an MGM rewards member, had reservations at Prime and Lago were we spend well over 1000 dollars. At the time of arrival, the person who helped us (Rosemary from Philippines) was not to helpful. She offered an upgarde to another room for an additional 450$ per night, even though as an Amex Platinum member we are supposed to receive this at no additional cost when available. On the second day of our stay, Aracelis was able to provide the service that was rightfully expected. In the original room that we were assigned (22 604) there was mold in the shower, dust on the furniture, drawers were dirty, shower head sprayed everywhere, there was glue and stains in the wall paper behind the bed specially. The quality of the toiletries was terrible')

sentiment_predict_rf('Dirty hallways')

sentiment_predict_rf('Dirty Room Stayed at the Bellagio for the first time ever for a work trip.')

# predict in test set
y_pred_rf = rf.predict(x_test)

# +
# classification result for test set
print('accuracy: %.2f' % accuracy_score(y_test, y_pred_rf))
print('precision: %.2f' % precision_score(y_test, y_pred_rf))
print('recall: %.2f' % recall_score(y_test, y_pred_rf))
print('F1: %.2f' % f1_score(y_test, y_pred_rf))

AUC_rf = roc_auc_score(y_test,y_pred_rf)
print('AUC: %.8f' % AUC_rf)
# -

# ### ê·¸ë¦¬ë“œ ì„œì¹˜

clf = RandomForestClassifier(random_state=0)
# min_impurity_decrease : ìµœì†Œ ë¶ˆìˆœë„(ì§€ë‹ˆê³„ìˆ˜)ê°€ ë‚®ë‹¤ = ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ë‹¤ = ë°ì´í„°ì˜ ê· ì¼ë„ê°€ ë†’ë‹¤  
params = {"max_depth": [1, 5, 10],
          "min_samples_split": [2, 3],
          "min_impurity_decrease": [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
grid_cv = GridSearchCV(clf, param_grid=params)
grid_cv.fit(x_train, y_train)

# ìµœì ì˜ í‰ê°€ íŒŒë¼ë¯¸í„°ëŠ” grid_cv.best_estimator_ì— ì €ì¥ë¨
print(grid_cv.best_params_, grid_cv.best_score_)# ê°€ì¥ ì í•©í•œ íŒŒë¼ë©”í„°, ìµœê³  ì •í™•ë„ í™•ì¸

# ## 6-5. KNN

# +
from sklearn.neighbors import KNeighborsRegressor

knn=KNeighborsRegressor()
knn.fit(x_train,y_train)


# -

def sentiment_predict_knn(input_text):

    #ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰
    input_text = data_text_cleaning(input_text)
    #input_text = ["".join(input_text)]

    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ í”¼ì²˜ ë²¡í„°í™”
    st_tfidf = cv.transform([input_text])
    tf_idf_vect = tfidf_vectorizer.transform(st_tfidf)

    # ìµœì  ê°ì„± ë¶„ì„ ëª¨ë¸ì— ì ìš©í•˜ì—¬ ê°ì„± ë¶„ì„ í‰ê°€
    st_predict = float(knn.predict(st_tfidf))

    #ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
    if(st_predict == 0):
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ë¶€ì • ê°ì„±')
    else :
        print('ì˜ˆì¸¡ ê²°ê³¼: ->> ê¸ì • ê°ì„±')


sentiment_predict_knn('it was nice')

sentiment_predict_knn('Hotel over all is very nice. Aracelis went above and beyond to assist us...... Im a Amex Platinum card holder, an MGM rewards member, had reservations at Prime and Lago were we spend well over 1000 dollars. At the time of arrival, the person who helped us (Rosemary from Philippines) was not to helpful. She offered an upgarde to another room for an additional 450$ per night, even though as an Amex Platinum member we are supposed to receive this at no additional cost when available. On the second day of our stay, Aracelis was able to provide the service that was rightfully expected. In the original room that we were assigned (22 604) there was mold in the shower, dust on the furniture, drawers were dirty, shower head sprayed everywhere, there was glue and stains in the wall paper behind the bed specially. The quality of the toiletries was terrible')

sentiment_predict_knn('Subpar Dirty hallways and less than average room.')

sentiment_predict_knn('Dirty Room Stayed at the Bellagio for the first time ever for a work trip.')

# predict in test set
y_pred_knn = knn.predict(x_test)

# +
# classification result for test set
# print('accuracy: %.2f' % accuracy_score(y_test, y_pred_knn))
# print('precision: %.2f' % precision_score(y_test, y_pred_knn))
# print('recall: %.2f' % recall_score(y_test, y_pred_knn))
# print('F1: %.2f' % f1_score(y_test, y_pred_knn))

# AUC Score í™•ì¸
AUC_knn = roc_auc_score(y_test,y_pred_knn)
print('AUC: %.8f' % AUC_knn)
# -

# ### ê·¸ë¦¬ë“œ ì„œì¹˜

# https://blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=baek2sm&logNo=221763552440&redirect=Dlog&widgetTypeCall=true&directAccess=false

clf = KNeighborsRegressor()
# min_impurity_decrease : ìµœì†Œ ë¶ˆìˆœë„(ì§€ë‹ˆê³„ìˆ˜)ê°€ ë‚®ë‹¤ = ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ë‹¤ = ë°ì´í„°ì˜ ê· ì¼ë„ê°€ ë†’ë‹¤  
params = [{'n_neighbors' : range(3,20)}]
grid_cv = GridSearchCV(clf, param_grid=params)
grid_cv.fit(x_train, y_train)

# ìµœì ì˜ í‰ê°€ íŒŒë¼ë¯¸í„°ëŠ” grid_cv.best_estimator_ì— ì €ì¥ë¨
print(grid_cv.best_params_, grid_cv.best_score_)# ê°€ì¥ ì í•©í•œ íŒŒë¼ë©”í„°, ìµœê³  ì •í™•ë„ í™•ì¸

# ## 6-6. LSTM

from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

embedding_dim = 100
hidden_units = 128

vocab_size = len(word_token)

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(1, activation='sigmoid'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(x_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64)

# ## 6-7. ê° ëª¨ë¸ë³„ AUC score ë¹„êµ

AUC_compare = [0.886423, 0.847511, 0.75893, 0.81440, 0.518946, 0.827861, 0.812024, 0.850780, 0.5]
AUC_compare

# +
plt.bar(range(len(AUC_compare)), AUC_compare)
plt.title("AUC score per model") #ì°¨íŠ¸ ì œëª©
plt.ylabel('AUC')
plt.ylim([0, 1])
plt.xticks([0,1,2,3,4, 5, 6, 7, 8], ['LR', 'SVM', 'DT', 'RF', 'KNN', 'GBM', 'XGB', 'CB', 'LSTM'])
plt.figure(figsize=(300,30))

plt.show()
# -

# # 7. ê°œì„ ì 

# 1. ê¸ì •/ë¶€ì • í‚¤ì›Œë“œì— ëŒ€í•œ ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ì—¬ ê²°ê³¼ë¥¼ ë³´ë‹¤ ê°€ì‹œì ìœ¼ë¡œ í‘œí˜„í•  ê²ƒ
# 2. RNN ê¸°ë°˜ì˜ LSTM ëª¨ë¸ì„ ì¶”ê°€í•˜ì—¬ ì •í™•ë„ë¥¼ ë³´ë‹¤ ë†’ì¼ ê²ƒ, ì´ë•Œ í‰ê°€ ì§€í‘œë¡œ AUC score ëŒ€ì‹  ë‹¤ë¥¸ ê²ƒì„ ìƒê°í•´ ë³¼ ê²ƒ 
